\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{times}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem,kantlipsum}
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[margin=0.9in]{geometry}
% \usepackage{titling}
\usepackage{authblk}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{bibentry}
\bibliographystyle{abbrv}


% Setup for header
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead[L]{Dian Wang} % Left side of the header
\fancyhead[C]{{List of Publications}} % Center of the header
\fancyhead[R]{wang.dian@northeastern.edu} % Right side of the header
\renewcommand{\headrulewidth}{0.4pt} % Add a horizontal line below the header

% Enable page number in the footer
% \fancyfoot[C]{\thepage} % Page number in the center of the footer
\hypersetup{
    colorlinks=true,
    linkcolor=NavyBlue,
    filecolor=magenta,      
    urlcolor=NavyBlue,
    citecolor=NavyBlue,
    }
    
\titlespacing*{\section}
{0pt}{2pt}{2pt}
\titlespacing*{\subsection}
{0pt}{2pt}{2pt}
\titlespacing*{\subsubsection}
{0pt}{2pt}{2pt}
\titlespacing*{\paragraph}
{0pt}{2pt}{2pt}

\titleformat{\section}
  {\normalfont\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}

% remove parindent, squeeze grafs
\setlength{\parindent}{0in}
\setlength{\parskip}{1ex}

\renewcommand\Affilfont{\small}

\newcommand{\paperItem}[2]{
  \item[#1]\small{
    {#2 \vspace{-5pt}}
  }
}

\title{Research Statement}
\author{Dian Wang}
\affil{Khoury College of Computer Sciences\\
Northeastern University}
\date{}

\begin{document}

\nobibliography{dian_wang_cv}
\textsc{Conference Papers}
\vspace{-0.1cm}
\begin{enumerate}
\paperItem{C19}{\bibentry{wang2024diffusion}}

% \textbf{Abstract}: Recent work has shown diffusion models are an effective approach to learning the multimodal distributions arising from demonstration data in behavior cloning. However, a drawback of this approach is the need to learn a denoising function, which is significantly more complex than learning an explicit policy. In this work, we propose Equivariant Diffusion Policy, a novel diffusion policy learning method that leverages domain symmetries to obtain better sample efficiency and generalization in the denoising function. We theoretically analyze the SO(2) symmetry of full 6-DoF control and characterize when a diffusion model is SO(2)-equivariant. We furthermore evaluate the method empirically on a set of 12 simulation tasks in MimicGen, and show that it obtains a success rate that is, on average, 21.9\% higher than the baseline Diffusion Policy. We also evaluate the method on a real-world system to show that effective policies can be learned with relatively few training samples, whereas the baseline Diffusion Policy cannot.
\paperItem{C18}{\bibentry{hu2024orbit}}
\paperItem{C17}{\bibentry{huang2024imagination}}
\paperItem{C16}{\bibentry{huang2023fourier}}
\paperItem{C15}{\bibentry{wang2023theory}}
\paperItem{C14}{\bibentry{nguyen2023equivariant}}
\paperItem{C13}{\bibentry{wang2023surprising}}
\paperItem{C12}{\bibentry{jia2023seil}}
\paperItem{C11}{\bibentry{huang2023edge}}
\paperItem{C10}{\bibentry{wang2022onrobot}}
\paperItem{C9}{\bibentry{nguyen2022leveraging}}
\paperItem{C8}{\bibentry{wang2022bulletarm}}
\paperItem{C7}{\bibentry{huang2022equivariant}}
\paperItem{C6}{\bibentry{zhu2022sample}}
\paperItem{C5}{\bibentry{wang2022so2}}

% \textbf{Abstract}: Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of Q-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal Q-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems.
\paperItem{C4}{\bibentry{wang2021equivariant}}
\paperItem{C3}{\bibentry{biza2021action}}
\paperItem{C2}{\bibentry{wang2020policy}}
\paperItem{C1}{\bibentry{wang2019assistive}}
\end{enumerate}

\textsc{Journal Papers}
\vspace{-0.1cm}
\begin{enumerate}
\paperItem{J3}{\bibentry{huang2023leveraging}}
\paperItem{J2}{\bibentry{zhu2023grasp}}
\paperItem{J1}{\bibentry{wilkinson2021design}}
\end{enumerate}

\textsc{Preprints}
\vspace{-0.1cm}
\begin{enumerate}
\paperItem{P3}{\bibentry{huang2024match}}
\paperItem{P2}{\bibentry{tangri2024equivariant}}
% \paperItem{P3}{\bibentry{zhu2024se3}}
\paperItem{P1}{\bibentry{jia2024open}}
% \paperItem{P1}{\bibentry{klee2024reducing}}
\end{enumerate}


\end{document}
